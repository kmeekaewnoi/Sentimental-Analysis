---
title: "Amazon Fine Food Reviews"
Name: Kesinee Meekaewnoi
output:
  word_document: default
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default

---


Exploring and preparing the data ---- 
read the Reviews data into the review data frame

**Data** The Review data has 10 columns and 568,454 observations.  
```{r }
review_raw<-read.csv("./dataset/amazon_reviews.csv",stringsAsFactors = FALSE)
head(review_raw)
```
Examine the structure of the Reviews data.
The only column that we are interested in are Score which is the product rating score from 1 to 5, and 
summary which are the customer reviews in text.
```{r }
str(review_raw)
```
Look at the distribution of the score
```{r}
barplot(table(review_raw$Score))
```
Examine the type variable more carefully.
```{r }
str(review_raw$Score)
table(review_raw$Score)
```
Combine review scores together and convert them to factor
```{r}
review_raw$rating <- ifelse(review_raw$Score < 3, 'bad', 'good')
review_raw$rating[review_raw$Score == 3] <- 'normal'
review_raw$rating <- as.factor(review_raw$rating)
```
Look at the review rating after we combined them
```{r}
head(review_raw$rating)
```

Build a corpus using the text mining (tm) package.
```{r }
#install.packages("tm")
library(tm)
review_corpus <- VCorpus(VectorSource(review_raw$Summary))
```
Examine the review corpus
```{r eval=FALSE}
print(review_corpus)
inspect(review_corpus[1:2])
```
```{r }
as.character(review_corpus[[1]])
lapply(review_corpus[1:2], as.character)
```
Clean up the corpus using tm_map()
```{r }
review_corpus_clean <- tm_map(review_corpus, content_transformer(tolower))

```
Show the difference between review_corpus and corpus_clean
```{r }
as.character(review_corpus[[1]])
as.character(review_corpus_clean[[1]])
```
Remove numbers, stop words, and punctuation.

```{r }
review_corpus_clean <- tm_map(review_corpus_clean, removeNumbers) # remove numbers
review_corpus_clean <- tm_map(review_corpus_clean, removeWords, stopwords()) # remove stop words
review_corpus_clean <- tm_map(review_corpus_clean, removePunctuation) # remove punctuation
```
Illustration of word stemming
```{r }
#install.packages("SnowballC")
library(SnowballC)
review_corpus_clean <- tm_map(review_corpus_clean, stemDocument)
review_corpus_clean <- tm_map(review_corpus_clean, stripWhitespace) # eliminate unneeded whitespace
```
Examine the final clean corpus
```{r }
lapply(review_corpus[1:3], as.character)
lapply(review_corpus_clean[1:3], as.character)
```
Create a document-term sparse matrix
```{r }
review_dtm <- DocumentTermMatrix(review_corpus_clean)
```
Creating training and test datasets
```{r }
review_dtm_train <- review_dtm[1:26299, ]
review_dtm_test  <- review_dtm[26300:35173, ]
```
Save the labels
```{r }
review_train_labels <- review_raw[1:26299, ]$rating
review_test_labels  <- review_raw[26300:35173, ]$rating
```
Check that the proportion of each rating score 
```{r }

prop.table(table(review_train_labels))
prop.table(table(review_test_labels))
```
Word cloud visualization

```{r }
#install.packages("wordcloud")
library(wordcloud)
wordcloud(review_corpus_clean, min.freq = 50, random.order = FALSE)
```
Subset the training data into rating score groups
```{r }
good <- subset(review_raw, rating == "good")
normal<- subset(review_raw, rating == "normal")
bad <- subset(review_raw, rating == "bad")
```

Word cloud visualization of each rating score
```{r }
wordcloud(good$Summary, max.words = 40, scale = c(3, 0.5))
wordcloud(normal$Summary, max.words = 40, scale = c(3, 0.5))
wordcloud(bad$Summary, max.words = 40, scale = c(3, 0.5))
```
```{r eval=FALSE}
review_dtm_freq_train <- removeSparseTerms(review_dtm_train, 0.999)
review_dtm_freq_train
```
Indicator features for frequent words
```{r eval=FALSE}
findFreqTerms(review_dtm_train, 5)
```
Save frequently-appearing terms to a character vector
```{r }
review_freq_words <- findFreqTerms(review_dtm_train, 5)
str(review_freq_words)
```
Create DTMs with only the frequent terms
```{r }
review_dtm_freq_train <- review_dtm_train[ , review_freq_words]
review_dtm_freq_test <- review_dtm_test[ , review_freq_words]
```
Convert counts to a factor
```{r }
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

```
Apply() convert_counts() to columns of train/test data
```{r }
review_train <- apply(review_dtm_freq_train, MARGIN = 2, convert_counts)
review_test  <- apply(review_dtm_freq_test, MARGIN = 2, convert_counts)

```
**Step 3**: Training a model on the data
```{r }
#install.packages("e1071")
library(e1071)
review_classifier<-naiveBayes(review_train,review_train_labels)
```
**Step 4**: Evaluating model performance
```{r }
review_test_predict<-predict(review_classifier,review_test)
head(review_test_predict)
```

```{r }
head(review_test_predict)
```
Confusion Matrix

```{r}
table(review_test_predict,review_test_labels)

```
Accuracy
```{r}
sum(review_test_predict==review_test_labels)/nrow(review_test)
```


```{r }
#install.packages("gmodels")
library(gmodels)
CrossTable(review_test_predict,review_test_labels,
           prob.chisq =FALSE, prob.t = FALSE, prob.r = FALSE,
           dnn = c('predict','actual'))

```
```{r}
(582+6515+95)/(582+6515+95+284+93+660+422+65+158)
```





**Step 5**: Improving model performance
```{r }
review_classifier2<-naiveBayes(review_train,review_train_labels,laplace = 1)
review_test_predict2<-predict(review_classifier2,review_test)
CrossTable(review_test_predict2,review_test_labels,
           prob.chisq =FALSE, prob.t = FALSE, prob.r = FALSE,
           dnn = c('predict','actual'))
```
Find the accuracy
```{r}
(593+6657+59)/(593+6657+59+207+93+667+458+47+93)
```
























